{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f571b1",
   "metadata": {},
   "source": [
    "# Hybrid Code Clustering (GPU-Optimized) — CodeBERT + AST/Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a34741",
   "metadata": {},
   "source": [
    "This notebook enables CUDA for CodeBERT fine-tuning and embedding; optional RAPIDS for GPU clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 0) REQUIREMENTS (run once if needed)\n",
    "# !pip install -U sentence-transformers transformers accelerate scikit-learn pandas numpy tqdm\n",
    "# Optional (GPU clustering with RAPIDS cuML; install per your CUDA/OS):\n",
    "# !pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
    "# !pip install umap-learn hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: C:\\files_dolab\\vfiles\n",
      "correct dir exists: True incorrect dir exists: True\n",
      "Output dir: outputs_hybrid_gpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1) CONFIG\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path(r\"D:\\GitHub\\dolap\\files_dolab\\files\")\n",
    "CORRECT_DIR = Path(r\"C:\\files_dolab\\dolap\\testfile\")\n",
    "INCORRECT_DIR = Path(r\"C:\\files_dolab\\dolap\\testvfile\")\n",
    "OUT_DIR = Path(\"outputs_hybrid_gpu\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DO_FINE_TUNE = True\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 24\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "BASE_MODEL = \"microsoft/codebert-base\"\n",
    "FT_MODEL_DIR = OUT_DIR / \"codebert_embed_contrastive_selfaug_gpu\"\n",
    "USE_RAPIDS = True\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"correct dir exists:\", CORRECT_DIR.exists(), \"incorrect dir exists:\", INCORRECT_DIR.exists())\n",
    "print(\"Output dir:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a491dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2) GPU CHECK\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3) LOAD + AUGMENT\n",
    "import re, random\n",
    "from pathlib import Path\n",
    "\n",
    "def load_py_with_paths(folder: Path):\n",
    "    items = []\n",
    "    for p in folder.rglob(\"*.py\"):\n",
    "        try:\n",
    "            txt = Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        items.append({\"path\": str(p), \"text\": txt})\n",
    "    return items\n",
    "\n",
    "def aug_minimal(code: str) -> str:\n",
    "    x = re.sub(r\"(?m)#.*$\", \"\", code)\n",
    "    x = re.sub(r\"(\\\"\\\"\\\"|\\'\\'\\')(?:.|\\n)*?\\1\", \"\", x)\n",
    "    x = re.sub(r\"[ \\t]+\", \" \", x)\n",
    "    x = re.sub(r\"\\n{3,}\", \"\\n\\n\", x).strip()\n",
    "    return x\n",
    "\n",
    "COMMON_RENAMES = [(r\"\\btemp\\b\",\"varA\"), (r\"\\bdata\\b\",\"varB\"), (r\"\\bi\\b\",\"idx\"), (r\"\\bj\\b\",\"jdx\"), (r\"\\bresult\\b\",\"outVal\")]\n",
    "def aug_rename_vars_light(code: str) -> str:\n",
    "    x = code\n",
    "    for pat, rep in COMMON_RENAMES:\n",
    "        x = re.sub(pat, rep, x)\n",
    "    return x\n",
    "\n",
    "rows_correct = load_py_with_paths(CORRECT_DIR) if CORRECT_DIR.exists() else []\n",
    "rows_incorrect = load_py_with_paths(INCORRECT_DIR) if INCORRECT_DIR.exists() else []\n",
    "print(f\"Loaded files — correct: {len(rows_correct)} | incorrect: {len(rows_incorrect)}\")\n",
    "\n",
    "pairs = []\n",
    "for row in rows_correct + rows_incorrect:\n",
    "    a1 = aug_minimal(row[\"text\"]); a2 = aug_rename_vars_light(row[\"text\"])\n",
    "    pairs.append((a1, a2))\n",
    "print(\"Positive pairs prepared:\", len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 4) SELF-CONTRASTIVE FINE-TUNING (GPU)\n",
    "from datetime import datetime\n",
    "if DO_FINE_TUNE and len(pairs) > 0:\n",
    "    from sentence_transformers import SentenceTransformer, models, InputExample, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "    word = models.Transformer(BASE_MODEL, max_seq_length=MAX_SEQ_LEN)\n",
    "    pool = models.Pooling(word.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "    sbert = SentenceTransformer(modules=[word, pool], device=DEVICE)\n",
    "    train_examples = [InputExample(texts=[a,b]) for a,b in pairs]\n",
    "    train_dl = DataLoader(train_examples, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=(DEVICE=='cuda'))\n",
    "    loss_fn = losses.MultipleNegativesRankingLoss(sbert)\n",
    "    warmup_steps = int(0.1 * len(train_dl) * EPOCHS)\n",
    "    print(\"Starting fine-tune on\", DEVICE, \"...\", datetime.now())\n",
    "    sbert.fit(train_objectives=[(train_dl, loss_fn)], epochs=EPOCHS, warmup_steps=warmup_steps, use_amp=True, show_progress_bar=True)\n",
    "    sbert.save(str(FT_MODEL_DIR))\n",
    "    print(\"Saved fine-tuned model →\", FT_MODEL_DIR)\n",
    "else:\n",
    "    print(\"[SKIP] Fine-tune disabled or no data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 5) EMBEDDING (GPU)\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "texts = [r[\"text\"] for r in rows_correct + rows_incorrect]\n",
    "labels = [1]*len(rows_correct) + [0]*len(rows_incorrect)\n",
    "paths  = [r[\"path\"] for r in rows_correct + rows_incorrect]\n",
    "if len(texts) == 0:\n",
    "    raise SystemExit(\"No .py files found.\")\n",
    "\n",
    "if FT_MODEL_DIR.exists():\n",
    "    emb_model = SentenceTransformer(str(FT_MODEL_DIR), device=DEVICE)\n",
    "    print(\"Loaded fine-tuned model:\", FT_MODEL_DIR)\n",
    "else:\n",
    "    emb_model = SentenceTransformer(modules=[models.Transformer(BASE_MODEL, max_seq_length=MAX_SEQ_LEN), models.Pooling(768, pooling_mode_mean_tokens=True)], device=DEVICE)\n",
    "    print(\"Loaded base model:\", BASE_MODEL)\n",
    "\n",
    "EMB = emb_model.encode(texts, normalize_embeddings=True, batch_size=64 if DEVICE=='cuda' else 16, show_progress_bar=True, device=DEVICE).astype(\"float32\")\n",
    "np.save(OUT_DIR/\"embeddings.npy\", EMB)\n",
    "np.save(OUT_DIR/\"labels.npy\", np.array(labels, dtype=np.int32))\n",
    "with open(OUT_DIR/\"paths.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in paths: f.write(p + \"\\\\n\")\n",
    "print(\"Saved embeddings/labels/paths →\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc196400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6) AST/RULE FEATURES (CPU)\n",
    "import ast, json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def ast_counts(src: str):\n",
    "    try:\n",
    "        t = ast.parse(src)\n",
    "    except Exception:\n",
    "        return {}, 0\n",
    "    c = Counter()\n",
    "    class V(ast.NodeVisitor):\n",
    "        def generic_visit(self, node):\n",
    "            c[type(node).__name__] += 1\n",
    "            super().generic_visit(node)\n",
    "    V().visit(t)\n",
    "    return {f\"AST_{k}\": v for k, v in c.items()}, 1\n",
    "\n",
    "def rule_signals(src: str):\n",
    "    s = src.lower()\n",
    "    return {\n",
    "        \"uses_match\": int(\"match \" in s),\n",
    "        \"uses_eval\": int(\"eval(\" in s),\n",
    "        \"uses_try\": int(\"try:\" in s),\n",
    "        \"uses_dict\": int(\"{\" in src and \"}\" in src and \":\" in src),\n",
    "        \"has_zero_check_literal\": int(\"== 0\" in s or \"==0\" in s or \"!= 0\" in s or \"!=0\" in s),\n",
    "        \"has_zero_check_except\": int(\"zerodivisionerror\" in s),\n",
    "        \"len_chars\": len(src),\n",
    "        \"len_lines\": src.count(\"\\\\n\") + (1 if src else 0),\n",
    "    }\n",
    "\n",
    "struct_rows = []\n",
    "for p, t in zip(paths, texts):\n",
    "    cnts, ok = ast_counts(t); sigs = rule_signals(t)\n",
    "    row = {\"path\": p, **cnts, **sigs}; struct_rows.append(row)\n",
    "\n",
    "df_struct = pd.DataFrame(struct_rows).fillna(0)\n",
    "df_struct.to_csv(OUT_DIR/\"struct_features.csv\", index=False)\n",
    "print(\"Saved structural features →\", OUT_DIR/\"struct_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 7) HYBRID + (Optional) GPU CLUSTERING (RAPIDS) ==========================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "HAS_CUML = False\n",
    "try:\n",
    "    if USE_RAPIDS:\n",
    "        from cuml.cluster import KMeans as cuKMeans\n",
    "        from cuml.manifold import UMAP as cuUMAP\n",
    "        HAS_CUML = True\n",
    "except Exception:\n",
    "    HAS_CUML = False\n",
    "\n",
    "E = np.load(OUT_DIR/\"embeddings.npy\")\n",
    "df_struct = pd.read_csv(OUT_DIR/\"struct_features.csv\")\n",
    "with open(OUT_DIR/\"paths.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    saved_paths = [line.strip() for line in f]\n",
    "\n",
    "num_cols = [c for c in df_struct.columns if c != \"path\"]\n",
    "X_struct = df_struct[num_cols].to_numpy(dtype=\"float32\")\n",
    "X_struct = np.nan_to_num(X_struct, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_z = scaler.fit_transform(X_struct)\n",
    "\n",
    "X = np.hstack([E, X_struct_z]).astype(\"float32\")\n",
    "np.save(OUT_DIR/\"hybrid_features.npy\", X)\n",
    "\n",
    "# Reducer\n",
    "if HAS_CUML and X.shape[0] >= 5:\n",
    "    umap = cuUMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    Xr = umap.fit_transform(X)\n",
    "    reducer_name = \"cuML-UMAP(2D)\"\n",
    "else:\n",
    "    pca = PCA(n_components=min(32, X.shape[1]), random_state=42)\n",
    "    Xr = pca.fit_transform(X)\n",
    "    reducer_name = f\"PCA({Xr.shape[1]})\"\n",
    "\n",
    "# Clustering\n",
    "if HAS_CUML and Xr.shape[0] >= 2:\n",
    "    best_k, best_inertia, best_labels = None, float(\"inf\"), None\n",
    "    for k in range(2, min(12, max(2, Xr.shape[0]-1))+1):\n",
    "        km = cuKMeans(n_clusters=k, random_state=42)\n",
    "        labs = km.fit_predict(Xr).to_array()\n",
    "        if km.inertia_ < best_inertia:\n",
    "            best_inertia, best_k, best_labels = km.inertia_, k, labs\n",
    "    labels_cluster, alg = best_labels, f\"cuML-KMeans(k={best_k}) + {reducer_name}\"\n",
    "else:\n",
    "    best_k, best_s, best_labels = None, -1, None\n",
    "    for k in range(2, min(12, max(2, Xr.shape[0]-1))+1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42).fit(Xr)\n",
    "        labs = km.labels_\n",
    "        if len(set(labs)) < 2: continue\n",
    "        try:\n",
    "            s = silhouette_score(Xr, labs)\n",
    "        except Exception:\n",
    "            s = -1\n",
    "        if s > best_s:\n",
    "            best_s, best_k, best_labels = s, k, labs\n",
    "    labels_cluster = best_labels if best_labels is not None else KMeans(n_clusters=2, n_init=\"auto\", random_state=42).fit_predict(Xr)\n",
    "    alg = f\"CPU-KMeans(k={best_k}) + {reducer_name}\"\n",
    "\n",
    "print(\"Clustering algorithm:\", alg)\n",
    "\n",
    "df_clusters = pd.DataFrame({\"path\": saved_paths, \"cluster\": labels_cluster, \"dim1\": Xr[:,0], \"dim2\": Xr[:,1] if Xr.shape[1] > 1 else 0.0})\n",
    "df_clusters.to_csv(OUT_DIR/\"hybrid_clusters.csv\", index=False)\n",
    "print(\"Saved →\", OUT_DIR/\"hybrid_clusters.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
